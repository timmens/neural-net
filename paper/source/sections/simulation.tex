\section{Simulation}
\label{seq:simulation}

In this section, I discuss the simulation part of this paper. First, I discuss the study
design, the employed fitting methods, and its results. Then I talk about the
difficulties of implementing industry-standard machine learning methods. All computer
codes needed to reproduce the simulation are available in the online repository
\url{http://github.com/timmens/neural-net}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Study Design
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Study Design}

The simulation is run over three different data generating process types and three
levels of the sample size $n \in \{100, 1.000, 10.000\}$. For each specification, I
simulate $100$ iterations. In each iteration, a model is fitted using training data of
size $n$ and its predictions are evaluated on a large testing set that is simulated once
for each specification. I measure the average square error of the predictions on the
testing set. The following data generating processes are implemented.

\paragraph{Linear}

For the linear model I simulate IID data with features $x_i \sim \mathcal{U}[-1, 1]^p$,
where $p = 30$ for all sample sizes.\footnote{By $\mathcal{U}[-1, 1]^p$ I denote the
$p$-dimensional multivariate uniform distribution on $[-1, 1]^p$ with independent
components.} The outcomes are computed using $y_i = \beta^\top x_i + e_i$, where $e_i
\sim \mathcal{N}(0, 1/4)$ and $\beta$ is drawn randomly once such that its $\ell_2$-norm
is 1.

\paragraph{Linear High-dimensional}

For the high-dimensional case I simulate IID data with features $x_i \sim
\mathcal{U}[-1, 1]^p$, where $p = \lfloor n / 10 \rfloor$. To incorporate sparsity I
randomly select $10\%$ of the features to be informative. Let $x_{i\mathcal{I}}$ denote
the vector containing the informative features. Then the outcomes are computed using
$y_i = \beta^\top x_{i\mathcal{I}} + e_i$, where $e_i \sim \mathcal{N}(0, 1/4)$.

\paragraph{Nonlinear High-dimensional}

For the nonlinear high-dimensional case I simulate IID data with features $x_i \sim
\mathcal{U}[-1, 1]^p$, where $p = \lfloor n / 10 \rfloor$. To incorporate sparsity I
select $10\%$ of the features to be informative. Let $x_{i\mathcal{I}}$ denote the
vector containing the informative features. Then the outcomes are computed using
$y_i = \prod_{j\in\mathcal{I}} x_{ij} + e_i$, where $e_i \sim \mathcal{N}(0, 1/4)$ and
$\beta$ is drawn randomly once for each parameter dimension such that its $\ell_2$-norm
is 1.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Methods}

I employ four different fitting methods in this study. They are listed in Table
\ref{tab:methods}.

\begin{table}
\begin{tabular}{r | p{0.8\linewidth}}
    \textit{OLS} & An ordinary least squares model with intercept. \\
    \textit{Lasso} & A Lasso model with intercept that uses $3$-fold cross-validation to
        determine the penalty parameter over the grid $(0.1, 0.001, 0.0001)$.  \\
    \textit{Net} & A neural network with ReLU activation functions and two hidden
        layers, where the first layer has $p/2$ nodes and the second $p/4$. The network
        is trained over 100 epochs using the Adam optimizer.  \\
    \textit{RegNet} & A neural network with ReLU activation functions and five hidden
        layers with $\max(2, \lfloor s \cdot p \rfloor)$ nodes, where $s \in (0, 1)$ is
        a sparsity level set to $0.01$. Further, the weights connecting the input layer
        and first hidden layer are penalized using an $\ell_1$-norm. The penalty
        parameter is set to $0.05$. The network is trained over 100 epochs using the
        Adam optimizer. \\
    \textit{Boosting} & A boosting algorithm with tree weak-learners of depth 2. The
        algorithm is run over 1.000 iterations. In each split in the regression tree
        a randomly selected subset of $20\%$ of the features is considered.
\end{tabular}
\caption{List of methods used in the simulation study.}
\label{tab:methods}
\end{table}

\begin{remark}
    Given the sparse nature of the high-dimensional data generating processes, one could
    argue that incorporating this information into the regularized network via a
    sparsity level parameter is unfair. This is a design choice I had to take since
    otherwise, the training would take too long; note that for $n = 10.000$ we have $p =
    1.000$ features, which combined with two hidden layers of size $500$ and $250$ would
    yield $625.000$ parameters, which cannot be trained in a simulation context on a
    standard computer.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results}

In Figure \ref{fig:simulation} the results are visualized. The rows correspond to the
DGP types, whereas the columns, correspond to the sample sizes. The figure visualizes
the estimated densities over the simulated iterations of the computed prediction mean
squared error on the test set. Because all DGPs had independent additive noise with a
variance of 0.25 we know that the minimal mean squared error is 0.25. I subtracted this
value from all terms so that the best possible performance is achieved at 0. Let me
briefly comment on three specifications. For the linear case --no matter the sample
size-- we see that OLS beats boosting and the neural network. This would be expected
since OLS uses the structure of the DGP. Similarly, for the linear high-dimensional case
(middle row), Lasso performs best. Interestingly, the regularized network is always the
worst. However, for the nonlinear high-dimensional case, OLS manages to beat the other
methods on average for the small sample size, but the regularized network beats all
other methods for the larger sample sizes.

With more computational resources, it would be interesting to analyze how different
types of architectural designs for the neural network would compare.

\begin{figure}
    \noindent\makebox[\textwidth]{%
    \includegraphics[scale=0.8]{../bld/simulation_result.png}}
    \caption{Simulation results: The rows correspond to DGP types \textit{Linear},
    \textit{Linear High-dimensional} and \textit{Nonlinear High-dimensional} (starting
    from the top row); whereas the columns correspond to the sample size in increasing
    order: $n = 100, 1.000, 10.000$ (starting from the left column). The linear
    correspond to density estimates over the simulation itertions.}
    \label{fig:simulation}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Implementation Remarks}

\paragraph{Training Time}

When implementing simulation exercises that use many samples and features, the fitting
machine learning algorithms can result in long running times. The training time of
state-of-the-art models can take weeks on special hardware. Running these algorithms on
Laptop CPUs can increase the training time, so that researchers cannot reproduce
industry-standard results since the training time becomes infeasible. In my simulation,
I had to choose relatively small networks to circumvent this problem. Even then, on my
2-core machine, the simulation ran for 10 hours. A simple solution is to run the code on
GPUs, which can produce speed-ups of up to 20.

\paragraph{Theory and Practice}

There is a further problem that hinders the soundness of classical simulation studies.
Modern machine learning algorithms are implemented modularly, and each component is
optimized to achieve good results even under default arguments, which sometimes set
hyper-parameters dependent on the data. Further, the actual implementation and
application of these algorithms usually deviate from the mathematical definition. This
loosens the connection between Monte Carlo results and theoretical arguments since the
researcher may not even be aware that a method is optimizing some hyper-parameter by
default. One could argue that this calls for libraries that only implement the
theoretical specification to create a more concrete connection between simulation
results and theoretical arguments; however, this would miss the point that in practice,
people use algorithms with complex defaults. This is a prime example where the gap
between theory and practice complicates a complete scientific analysis.

\paragraph{Custom Implementation}

Unrelated to the simulation study, I implemented a general neural network and its
fitting procedure to understand the technical details better. It is designed for
classification and regression and tested on the MNIST data set containing hand-written
digits. More details on this mini-project are described in the online repository.
