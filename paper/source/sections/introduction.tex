\section{Introduction}
\label{seq:introduction}

With the rise of cheap computing, research and application of computer-aided statistical
algorithm have skyrocketed. In particular machine learning, a sub-area of computational
statistics, that lays specific focus on using algorithms has experienced the greatest
increase in both research and its direct application in the industry. Given this success
other research areas are trying to figure out how to incorporate these successful
methods to improve their research; see for example \cite{Varian.2014} and
\cite{Athey.2019}. This tasks turns out harder than expected since research in economics
--or other disciplines-- is (usually) interested in inference, but machine learning is
(usually) used for prediction. Some authors accomplished to combine problems in
econometrics, in particular in causal inference, with machine learning methods. Most
notably is the paper series by Susan Athey, Stefan Wager and Guido Imbens, that solves
the estimation of heterogeneous treatment effects using tree-based machine learning
methods; see \cite{Athey.2016} and \cite{Athey.2018}. A different approach is taken in
the liteature on doubly robust estimation of structural parameters; see for example
\cite{Chernozhukov.2018}. The estimators studied there require the estimation of
nuisance functions under high-level assumptions on the convergence rates. Since these
nuisance functions are often conditional expectations this opens up the possibility to
use arbitrary machine learning methods for the estimation. But how do machine learning
methods fare in comparison to classical methods on prediction tasks commonly encountered
in economics?

The rest of this paper is structured as follows: In the remaining part of the
introduction I will provide a primer on neural networks. In section \ref{seq:fitting} I
take a closer look at fitting procedures for neural networks. I will also remark on
regularization strategies in neural networks. These parts can be skipped if the reader
is familiar with neural networks. In section \ref{seq:rates} I consider approximation
theorems and convergence rates. In section \ref{seq:simulation} I present a simulation
study comparing neural networks with linear models in different scenarios. Section
\ref{seq:conclusion} concludes.

% ======================================================================================
% A Primer on Neural Networks
% ======================================================================================

\subsection{A Primer on Neural Networks}

In the remaining parts of this section I provide a small primer on feed-forward neural
networks. I discuss the connection between their graphical and mathematical
representation, as well as common concepts such as the activation function. Fitting
techniques are presented in the next section. For a detailed description of neural
networks see \cite{Goodfellow.2016, Murphy.2012} or \cite{Hastie.2008}.

On a high-level, neural networks can be thought of as parametrized functions $f(\cdot;
\theta):\mathbb{R}^p \to \mathbb{R}^l$ mapping some feature vector $x \in \mathbb{R}^p$
to the outcome space, with (potentially) high-dimensional parameter vector $\theta \in
\Theta$, that ship with efficient procedures to calibrate this parameter on data.

The approximation quality of neural networks has been studied extensively in the
literature. Several universal approximation theorems have been proven which show that
under certain conditions on the network architecture they can approximate any continuous
function arbitrary well. This will be made more precise in section \ref{seq:rates}.

% ======================================================================================
% General Notion
% ======================================================================================
\paragraph{General Notion}

In the remainder of this sections let us assume that the parameter vector $\theta$
has already been estimated. How does a neural network generate predictions?

\begin{figure}[!ht]
\centering
\begin{tikzpicture}[shorten >=1pt,->,draw=black!60, node distance=\layersep, scale=1.1]

    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle, draw=black!80, minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron];
    \tikzstyle{output neuron}=[neuron];
    \tikzstyle{hidden neuron}=[neuron];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \y] (I-\name) at (0,-\y) {};

    % Draw the first hidden layer nodes
    \foreach \name / \y in {1,...,5}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

    % Draw the second hidden layer nodes
    \foreach \name / \y in {6/1,7/2,8/3,9/4,10/5}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (2 * \layersep,-\y cm) {};

    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-8] (O) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,5}
            \path (I-\source) edge (H-\dest);

    % Connect every node in the first hidden layer with every node in the
    % second hidden layer.
    \foreach \source in {1,...,5}
        \foreach \dest in {6,...,10}
            \path (H-\source) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {6,...,10}
        \path (H-\source) edge (O);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl1) {Hidden layer 1};
    \node[annot,above of=H-6, node distance=1cm] (hl2) {Hidden layer 2};
    \node[annot,left = 1.75cm of hl1] {Input layer};
    \node[annot,right = 0.5cm of hl2] {Output layer};
\end{tikzpicture}
\caption{Depiction of a neural network. The network has 4 inputs, two hidden layers
with 5 nodes each and a single output node.}
\label{fig:neural_net}
\end{figure}
Consider the depiction of a two-layer network in figure \ref{fig:neural_net}. This
neural network represents a function $f : \mathbb{R}^4 \to \mathbb{R}$. The input layer
represents the feature vector, and similarly the output layer represents the outcome
variable. The network is called two-layer because there are two hidden layers. The
number of nodes in each layer are called the depth of that layer, and the number of
hidden layers is called the depth of the network. In this paper I consider
fully-connected networks, which means that each node in a previous layer is connected
with each node of the next layer. To build the bridge to the mathematical formulation
let us focus on the connection between the input layer and a single node in the first
hidden layer. This is visualized in Figure \ref{fig:neural_net_single_connection}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}[shorten >=1pt,->,draw=black!25, node distance=\layersep, scale=1.5]

    \tikzstyle{every pin edge}=[<-,shorten <=1pt, draw=black!90]
    \tikzstyle{neuron}=[circle, draw=black!80, minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, draw=black!90];
    \tikzstyle{output neuron}=[neuron];
    \tikzstyle{hidden neuron}=[neuron, draw=black!90];
    \tikzstyle{hidden neuron background}=[neuron, draw=black!25];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \y] (I-\name) at (0,-\y) {$x_\name$};

    % Draw the first hidden layer nodes
    \path[yshift=0.5cm]
        node[hidden neuron] (H-2) at (\layersep,-2 cm) {$h$};
    \foreach \name / \y in {1,3,4,5}
        \path[yshift=0.5cm]
            node[hidden neuron background] (H-\name) at (\layersep,-\y cm) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,4}
        \foreach \dest in {1,3,4,5}
            \path (I-\source) edge (H-\dest);
    \foreach \source in {1,...,4}
        \path (I-\source) edge[draw=black!90] (H-2);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl1) {Hidden layer 1};
    \node[annot,left = 1.75cm of hl1] {Input layer};
\end{tikzpicture}

\caption{Depiction of the connection between the input layer and a single hidden node.}
\label{fig:neural_net_single_connection}
\end{figure}

Consider an arbitrary node $h$ from the first hidden layer. The value that $h$ attains
after the network receives an input $x = (x_1, x_2, x_3, x_4)^\top$, is defined as
\begin{align*}
    h(x) = \sigma(w^\top x + b),
\end{align*}
where $w \in \mathbb{R}^4$ denotes the weights of the edges and $b \in \mathbb{R}$ the
bias term, which is associated with each hidden node. $\sigma$ denotes the activation
function. Activation functions are motivated by biological processes in the brain. Human
neurons require its input signal to exceed some threshold before they themselves send
out new signals; see \cite{Goodfellow.2016}. Activation functions try to model this
behavior by outputting a low value when $w^\top x$ is smaller than the bias $b$, and
outputting a larger value when $w^\top x$ overcomes the bias. Traditionally the most
common activation function is the Sigmoid function $\sigma(z) = {1}/{(1 + \exp(-z))}$
which is depicted in Figure \ref{fig:sigmoid_function}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}[scale=1]
    \begin{axis}[
        axis on top = true,
        axis x line = bottom,
        axis y line = left,
        xlabel = $z$,
        ylabel = $\sigma$
    ]
        \addplot[
            domain = -7:7,
            samples = 100
        ]
            {1/(1+exp(-x))};

    \end{axis}
\end{tikzpicture}
\caption{Sigmoid function.}\label{fig:sigmoid_function}
\end{figure}

To compute the value of a node in the second hidden layer we need more notation.
Consider Figure \ref{fig:neural_net_double_connection} and let me now denote an
arbitrary node of the second hidden layer by $h$.
\begin{figure}[!ht]
\centering
\begin{tikzpicture}[shorten >=1pt,->,draw=black!25, node distance=\layersep, scale=1.5]

    \tikzstyle{every pin edge}=[<-,shorten <=1pt, draw=black!90]
    \tikzstyle{neuron}=[circle, draw=black!80, minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron];
    \tikzstyle{hidden neuron}=[neuron];
    \tikzstyle{hidden neuron background}=[neuron, draw=black!25];
    \tikzstyle{annot} = [text width=4em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,4}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \y] (I-\name) at (0,-\y) {$x_\name$};

    % Draw the first hidden layer nodes
    \foreach \name / \y in {1,...,5}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {$h_\name$};

    % Draw the second hidden layer nodes
    \path[yshift=0.5cm]
        node[hidden neuron] (H-7) at (2 * \layersep,-2 cm) {$h$};
    \foreach \name / \y in {6/1,8/3,9/4,10/5}
        \path[yshift=0.5cm]
            node[hidden neuron background] (H-\name) at (2 * \layersep,-\y cm) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,5}
            \path (I-\source) edge[draw=black!90] (H-\dest);

    % Connect every node in the first hidden layer with every node in the
    % second hidden layer.
    \foreach \source in {1,...,5}
        \path (H-\source) edge[draw=black!90] (H-7);
    \foreach \source in {1,...,5}
        \foreach \dest in {6,8,9,10}
            \path (H-\source) edge (H-\dest);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=1cm] (hl1) {Hidden layer 1};
    \node[annot,above of=H-6, node distance=1cm] (hl2) {Hidden layer 2};
    \node[annot,left = 1.75cm of hl1] {Input layer};
\end{tikzpicture}

\caption{Depiction of the connection between the input layer and a single node in the
second hidden layer.}
\label{fig:neural_net_double_connection}
\end{figure}
If we had already computed the values of the nodes $\mathbf{h}_1 = (h_1, \dots,
h_5)^\top$ the value of $h$ would be given by the same logic as before: $h =
\sigma(w_h^\top x + b_h)$. Here I represent the bias term for node $h$ as $b_h$ and the
weights connecting the first layer with node $h$ as $w_h$. Letting $w_{ij}$ denote the
weights connecting all nodes in layer $i$ with node $j$ of layer $i+1$, we know from
before that
\begin{align*}
    \mathbf{h}_1(x) :=
        \begin{bmatrix}
            \sigma(w_{11}^\top x)\\
            \vdots\\
            \sigma(w_{15}^\top x)
        \end{bmatrix}.
\end{align*}
We can simplify this by defining a weight matrix for each layer. For the first hidden
layer we have $W_1 := (w_{11} \dots, w_{15})^\top$. Allowing element-wise operations on
$\sigma$ we get $\mathbf{h}_1(x) := \sigma(W_1 x)$ and ultimately for the value of $h$
we have $h(x) = \sigma(w_h^\top \mathbf{h}_1(x) + b_h)$. Combining all of the above we
can write down the analytical representation of this neural network. Define $W_2$
analoguesly to $W_1$ and let $a_0$ and $a$ denote the bias term and weight vector for
the last layer, respectively. Then,
\begin{align*}
    f(x) &= a_0 + a^\top \mathbf{h}_2(x)\\
         &= a_0 + a^\top \sigma(W_2 \, \mathbf{h}_1(x))\\
         &= a_0 + a^\top \sigma \big( W_2 \, \sigma(W_1 \, x) \big).
\end{align*}


% ======================================================================================
% Extensions
% ======================================================================================
\paragraph{Extensions}

The network I presented above is the bare-bone version of neural networks. A popular
component of the model that is replaced by newer versions is the activation function;
a comparison is presented in \cite{Hara.1994}. The optimal choice of the activation
function depends on the problem at hand, but the state-of-the-art default is the
ReLU function, depicted in Figure \ref{fig:relu_function}. \cite{Hinton.2017} provide
empirical evidence that the ReLU function can improve upon the classical Sigmoid
function by accelerating the convergence of the fitting procedure.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}[scale=1]
    \begin{axis}[
        axis on top = true,
        axis x line = bottom,
        axis y line = left,
        xlabel = $z$,
        ylabel = $\sigma$,
        ymin=-0.5
    ]
    \addplot[
        domain=-7:7,
        samples=100
    ] {(x>=0)*x};
    \end{axis}
\end{tikzpicture}
\caption{Rectified linear unit (ReLU) activation function.}\label{fig:relu_function}
\end{figure}
