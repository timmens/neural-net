In this paper, I investigate the predictive performance of neural networks. I compare
unregularized and regularized networks, where in the latter case, I place an
$\ell_1$-type penalty on the weights. For the unregularized case, I discuss convergence
rates. Using a simulation study, I investigate whether neural networks can recover
sparse structures in high-dimensional settings and if their performance is superior to
other methods. I find that for nonlinear DGPs, neural networks outperform linear models
but are beaten by boosting, whereas, for linear DGPs, OLS and the Lasso beat neural
networks and boosting. Computer codes and additional materials can be found in the
online repository: \url{https://github.com/ timmens/neural-net}.
