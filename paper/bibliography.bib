w% Encoding: UTF-8



@Article{Sklearn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{Lee.2016,
  doi = {10.48550/ARXIV.1602.04915},
  url = {https://arxiv.org/abs/1602.04915},
  author = {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title = {Gradient Descent Converges to Minimizers},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Kingma.2014,
  doi = {10.48550/ARXIV.1412.6980},
  url = {https://arxiv.org/abs/1412.6980},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Adam: A Method for Stochastic Optimization},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@Inproceedings{Snoek.2012,
 author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Practical Bayesian Optimization of Machine Learning Algorithms},
 url = {https://proceedings.neurips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf},
 volume = {25},
 year = {2012}
}


@Article{Athey.2019,
author = {Athey, Susan and Imbens, Guido W.},
title = {Machine Learning Methods That Economists Should Know About},
journal = {Annual Review of Economics},
volume = {11},
number = {1},
pages = {685-725},
year = {2019},
doi = {10.1146/annurev-economics-080217-053433},
URL = {https://doi.org/10.1146/annurev-economics-080217-053433},
eprint = {https://doi.org/10.1146/annurev-economics-080217-053433},
abstract = { We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models. }
}


@Article{Athey.2018,
author = {Stefan Wager and Susan Athey},
title = {Estimation and Inference of Heterogeneous Treatment Effects using Random Forests},
journal = {Journal of the American Statistical Association},
volume = {113},
number = {523},
pages = {1228-1242},
year  = {2018},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2017.1319839},
URL = {https://doi.org/10.1080/01621459.2017.1319839},
eprint = { https://doi.org/10.1080/01621459.2017.1319839}
}


@Article{Athey.2016,
author = {Susan Athey  and Guido Imbens },
title = {Recursive partitioning for heterogeneous causal effects},
journal = {Proceedings of the National Academy of Sciences},
volume = {113},
number = {27},
pages = {7353-7360},
year = {2016},
doi = {10.1073/pnas.1510489113},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1510489113},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1510489113},
}


@Article{Varian.2014,
Author = {Varian, Hal R.},
Title = {Big Data: New Tricks for Econometrics},
Journal = {Journal of Economic Perspectives},
Volume = {28},
Number = {2},
Year = {2014},
Month = {May},
Pages = {3-28},
DOI = {10.1257/jep.28.2.3},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3}
}

@Article{Chernozhukov.2018,
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
title = {Double/debiased machine learning for treatment and structural parameters},
journal = {The Econometrics Journal},
volume = {21},
number = {1},
pages = {C1-C68},
doi = {https://doi.org/10.1111/ectj.12097},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097},
year = {2018}
}


@Article{Rumelhart.1986,
  title   = {Learning representations by back-propagating errors},
  author  = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal = {Nature},
  year    = {1986},
  volume  = {323},
  pages   = {533-536}
}

@Inproceedings{Caruana.2000,
 author    = {Caruana, Rich and Lawrence, Steve and Giles, C.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor    = {T. Leen and T. Dietterich and V. Tresp},
 publisher = {MIT Press},
 title     = {Overfitting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping},
 volume    = {13},
 year      = {2000}
}

@Inproceedings{Krogh.1991,
 author    = {Krogh, Anders and Hertz, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor    = {J. Moody and S. Hanson and R.P. Lippmann},
 pages     = {},
 publisher = {Morgan-Kaufmann},
 title     = {A Simple Weight Decay Can Improve Generalization},
 url       = {https://proceedings.neurips.cc/paper/1991/file/8eefcfdf5990e441f0fb6f3fad709e21-Paper.pdf},
 volume    = {4},
 year      = {1991}
}

@Article{Neumann.2000,
  author  = {{Michael H. Neumann} and {J\"{u}rgen Franke}},
  title   = {Bootstrapping Neural Networks},
  year    = {2000},
  volume  = {12},
  journal = {Neural Comput.},
  pages   = {1929-1949},
}

@Article{Cybenko.1989,
  title   = {Approximation by superpositions of a sigmoidal function},
  author  = {George V. Cybenko},
  journal = {Mathematics of Control, Signals and Systems},
  year    = {1989},
  volume  = {2},
  pages   = {303-314}
}

@Article{Hornik.1989,
  title   = {Multilayer feedforward networks are universal approximators},
  author  = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
  journal = {Neural Networks},
  year    = {1989},
  volume  = {2},
  pages   = {359-366}
}

@Article{Hornik.1991,
  title   = {Approximation capabilities of multilayer feedforward networks},
  author  = {Kurt Hornik},
  journal = {Neural Networks},
  year    = {1991},
  volume  = {4},
  pages   = {251-257}
}

@Article{Pinkus.1999,
  title   = {Approximation theory of the MLP model in neural networks},
  author  = {Allan Pinkus},
  journal = {Acta Numerica},
  year    = {1999},
  volume  = {8},
  pages   = {143 - 195}
}


@InProceedings{Kidger.2020,
  title     = {{Universal Approximation with Deep Narrow Networks}},
  author    = {Kidger, Patrick and Lyons, Terry},
  booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
  pages     = {2306--2327},
  year      = {2020},
  editor    = {Abernethy, Jacob and Agarwal, Shivani},
  volume    = {125},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--12 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v125/kidger20a/kidger20a.pdf},
  url       = {https://proceedings.mlr.press/v125/kidger20a.html},
}

@article{Hinton.2017,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  title = {ImageNet Classification with Deep Convolutional Neural Networks},
  year = {2017},
  issue_date = {June 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {60},
  number = {6},
  issn = {0001-0782},
  url = {https://doi.org/10.1145/3065386},
  doi = {10.1145/3065386},
  journal = {Commun. ACM},
  pages = {84â€“90},
  numpages = {7}
}

@INPROCEEDINGS{Hara.1994,
  author={K. Hara and K. Nakayamma},
  booktitle={Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)},
  title={Comparison of activation functions in multilayer neural network for pattern classification},
  year={1994},
  volume={5},
  pages={2997-3002 vol.5}
}

@Book{Goodfellow.2016,
  author    = {Goodfellow, Ian; Bengio, Yoshua and Courville, Aaron},
  title     = {Deep learning},
  year      = {2016},
  address   = {Cambridge, MA, USA},
  publisher = {MIT Press},
}

@Book{Murphy.2012,
  author    = {Kevin P. Murphy},
  title     = {Machine Learning: A Probabilistic Perspective},
  year      = {2012},
  publisher = {MIT press},
  address   = {Cambridge, MA},
}

@Book{Hastie.2008,
  author    = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
  title     = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  year      = {2008},
  publisher = {Springer},
  address   = {New York, NY},
}

@Inproceedings{Holena.2010,
author = {Holena, Martin and Linke, David and Rodemerck, Uwe and Bajer, Lukas},
year = {2010},
month = {06},
pages = {351-366},
title = {Neural Networks as Surrogate Models for Measurements in Optimization Algorithms},
isbn = {978-3-642-13567-5},
doi = {10.1007/978-3-642-13568-2_25},
booktitle = {},
}

@article{Gang.2019,
author = {Sun, Gang and Wang, Shuyue},
year = {2019},
month = {07},
pages = {095441001986448},
title = {A review of the artificial neural network surrogate modeling in aerodynamic design},
journal = {Proceedings of the Institution of Mechanical Engineers, Part G: Journal of Aerospace Engineering},
doi = {10.1177/0954410019864485}
}

@article{Tripathy.2018,
   title={Deep UQ: Learning deep neural network surrogate models for high dimensional uncertainty quantification},
   volume={375},
   ISSN={0021-9991},
   url={http://dx.doi.org/10.1016/j.jcp.2018.08.036},
   DOI={10.1016/j.jcp.2018.08.036},
   journal={Journal of Computational Physics},
   publisher={Elsevier BV},
   author={Tripathy, Rohit K. and Bilionis, Ilias},
   year={2018},
   month={Dec},
   pages={565â€“588}
}


@article{Ruppert.1985,
author = {Ruppert, David},
year = {1985},
month = {03},
pages = {},
title = {A Newton-Raphson Version of the Multivariate Robbins-Monro Procedure},
volume = {13},
journal = {The Annals of Statistics},
doi = {10.1214/aos/1176346589}
}


@article{Srivastava.2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = {01},
pages = {1929â€“1958},
numpages = {30},
keywords = {neural networks, regularization, deep learning, model combination}
}
